{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37f2fc2f",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a96cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3899d",
   "metadata": {},
   "source": [
    "# S√©quence d'Entra√Ænement\n",
    "\n",
    "L'entra√Ænement se fait en 4 phases it√©ratives, align√©es sur le framework LUCIDE pour unifier l'inf√©rence causale via un √©quilibre dynamique entre les distributions. Focus sur la coh√©rence bay√©sienne, en exploitant les GFlowNets pour g√©rer les espaces compositionnels et l'apprentissage distributionnel pour quantifier l'incertitude.\n",
    "\n",
    "### Phase 1: Environmental Grounding\n",
    "**Objectif** : Aligner les distributions de pr√©diction et de fr√©quence de l'environnement sur les observations environnementales.\n",
    "- Apprentissage de la prior environnementale $p^{env}_\\theta$ (mod√©lis√©e via GFlowNet, nomm√©e **llm_prior_env_model**) : Aligne sur les fr√©quences observ√©es.\n",
    "  - Optimisation : $\\theta^* = \\arg \\min_\\theta \\mathbb{E}_{x \\sim p^{env}} \\left[ \\log \\frac{p^{\\text{env}}(x)}{p^{env}_\\theta(x)}\\right]$\n",
    "- Apprentissage du mod√®le conditionnel Seq-to-Seq $p_\\phi^{LLM}(y|x)$ : Entra√Ænement autoregressif standard sur donn√©es environnementales.\n",
    "  - Optimisation : $\\phi^* = \\arg \\min_\\phi \\mathbb{E}_{(x,y) \\sim p^{\\text{env}}} \\left[ - \\log p_{\\phi}^{\\text{LLM}}(y|x) \\right]$\n",
    "\n",
    "### Phase 2: Internal Belief Consolidation\n",
    "**Objectif** : Mettre √† jour le syst√®me de croyances internes en utilisant les distributions apprises (analogue √† une exploration onirique des structures de croyances).\n",
    "- Apprentissage de la prior interne $p^{internal}_\\psi$ (mod√©lis√©e via GFlowNet, int√©gr√©e au prior unifi√© $p_{\\theta,\\psi}^{\\text{prior}} \\propto p_\\theta^{env} \\times p_\\psi^{internal}$).\n",
    "  - Cherche la coh√©rence entre prior et posterior : $p^{env}_\\theta(x) \\times p^{internal}_\\psi(x) \\times p_{\\phi}^{LLM}(y|x) \\propto p^{env}(x|y), \\quad \\forall x \\sim p^{internal}$\n",
    "  - Optimisation : $\\psi^* = \\arg \\min_\\psi \\mathbb{E}_{x \\sim p^{internal}_\\psi} \\left[ \\left(\\log \\frac{Z_\\psi^{\\text{internal}} \\times p_\\psi^{\\text{internal}}(x)}{R(x)}\\right)^2 \\right]$, o√π $R(x) = p^{\\text{env}}_\\theta(x) \\times p^{\\text{internal}}_\\psi(x) \\times p_\\phi^{\\text{LLM}} (y|x)$\n",
    "\n",
    "### Phase 3: Adversarial Exploration\n",
    "**Objectif** : D√©couvrir les s√©quences qui violent la coh√©rence bay√©sienne ‚Äì identifier les angles morts du raisonnement.\n",
    "- Apprentissage de la distribution adversariale $p_\\omega^{adv}$ (mod√©lis√©e via GFlowNet, nomm√©e LLM_ADVERSARIAL).\n",
    "  - G√©n√®re des contextes o√π le mod√®le √©choue : $p^{env}_\\theta(x) \\times p^{internal}_\\psi(x) \\times p_{\\phi}^{LLM}(y|x) \\not\\propto p^{env}(x|y), \\quad \\forall x \\sim p^{adv}_\\omega$\n",
    "  - Maximise la divergence bay√©sienne : $\\omega^* = \\arg \\max_\\omega \\mathbb{E}_{x \\sim p^{adv}_\\omega} \\left[ \\left(\\log \\frac{Z_\\omega^{\\text{adv}} \\times p_\\omega^{\\text{adv}}(x)}{R(x)}\\right)^2 \\right]$\n",
    "\n",
    "\n",
    "### Phase 4: Adversarial Correction\n",
    "**Objectif** : Restaurer la coh√©rence bay√©sienne sur les contextes adversariaux ‚Äì apprendre des erreurs.\n",
    "- Ajustement du mod√®le g√©n√©ratif Seq-to-Seq sur les exemples adversariaux.\n",
    "  - Optimisation : $\\phi^* = \\arg \\min_\\phi \\mathbb{E}_{x \\sim p^{\\text{adv}}_\\omega} \\left[ - \\log \\left( p^{\\text{env}}_\\theta(x) \\times p^{\\text{internal}}_\\psi(x) \\times p_\\phi^{\\text{LLM}}(y|x) \\right) \\right]$\n",
    "- Boucle de renforcement : Int√®gre les ajustements pour maximiser la vraisemblance marginale $p(y)$ sans calculer explicitement le posterior intractable.\n",
    "\n",
    "Il faut 4 fonctions de loss principales, adapt√©es aux phases :\n",
    "- $\\mathcal{L}_{Phase1}$ (pour $p^{env}_\\theta$ et Seq-to-Seq) : Bas√©e sur KL-divergence et cross-entropy.\n",
    "- $\\mathcal{L}_{Phase2}$ (pour $p^{internal}_\\psi$) : MSE sur log-ratios pour coh√©rence interne.\n",
    "- $\\mathcal{L}_{Phase3}$ (pour $p^{adv}_\\omega$) : Maximisation via MSE invers√©e.\n",
    "- $\\mathcal{L}_{Phase4}$ (pour Seq-to-Seq sur adversariaux) : N√©gative log-likelihood pond√©r√©e.\n",
    "\n",
    "Inputs : logprobs (B,), Z() pour les GFlowNets.\n",
    "\n",
    "## Formatage des Entr√©es/Sorties\n",
    "\n",
    "Adapt√© au dataset d'addition pour validation (a+b=c, a,b in [0,99], train/exclut [40,49], eval sur [40,49]). Vocab : digits 0-9, '+', '='. Utilise GFlowNets pour sampling compositionnel et RL distributionnel pour uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539875cc",
   "metadata": {},
   "source": [
    "# Model I/O Formatting\n",
    "\n",
    "### üß† LLM PRIOR ENV (GFlowNet)\n",
    "- **Input:** initial state  $s_0 = \\langle \\text{BOS} \\rangle$\n",
    "- **Output sequence example:**  $\\langle \\text{BOS} \\rangle \\, a\\, a\\, a \\, + \\, b\\, b\\, b \\, = \\langle \\text{EOS} \\rangle$\n",
    "- **Max length:**  `LLM_PRIOR_ENV_MAX_LEN_PRED = 13`\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Seq-to-Seq Model\n",
    "- **Encoder input:** cleaned sequence (no BOS) $\\text{\"aaa + bbb =<EOS>\"}$\n",
    "  ‚Üí `SEQ_2_SEQ_MAX_LEN_ENCODER = 12`\n",
    "- **Decoder input:** forced BOS token $\\langle \\text{BOS} \\rangle \\text{\"xxxx\"}$\n",
    "  ‚Üí `SEQ_2_SEQ_MAX_LEN_DECODER = 5`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° LLM ADVERSARIAL (GFlowNet)\n",
    "- **Input:**  $s_0 = \\langle \\text{BOS} \\rangle$\n",
    "- **Output sequence example:**  $\\langle \\text{BOS} \\rangle \\, a\\, a\\, a \\, + \\, b\\, b\\, b \\, = \\langle \\text{EOS} \\rangle$\n",
    "- **Max length:**  \n",
    "  `LLM_PRIOR_ENV_MAX_LEN_PRED = 13`\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Model Outputs\n",
    "\n",
    "Each model returns a tuple: $(\\log p(\\text{seq}) \\in \\mathbb{R}^B,\\; \\ln Z \\in \\mathbb{R})$\n",
    "\n",
    "where:\n",
    "- **$\\log p(\\text{seq})$**: sequence log-probability (per batch),\n",
    "- **$\\ln Z$**: learnable normalization scalar (partition energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d53e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_LLM_PRIOR_ENV(out_LLM_PRIOR_ENV, out_Seq2Seq):\n",
    "    \"\"\"\n",
    "    Compute the Trajectory Balance loss for the LLM PRIOR ENV (GFlowNet) phase.\n",
    "\n",
    "    Args:\n",
    "        out_LLM_PRIOR_ENV: tuple(logp_LLM_PRIOR_ENV (B,), lnZ_LLM_PRIOR_ENV scalar)\n",
    "        out_Seq2Seq: logp_Seq2Seq (B,)\n",
    "\n",
    "    Returns:\n",
    "        loss_LLM_PRIOR_ENV: scalar tensor (for backprop on LLM_PRIOR_ENV)\n",
    "    \"\"\"\n",
    "    logp_LLM_PRIOR_ENV, lnZ_LLM_PRIOR_ENV = out_LLM_PRIOR_ENV\n",
    "    logp_Seq2Seq_detached = out_Seq2Seq.detach()\n",
    "\n",
    "    # Reward = logp_prior_env + logp_seq2seq (detached)\n",
    "    reward = logp_LLM_PRIOR_ENV + logp_Seq2Seq_detached\n",
    "\n",
    "    # Trajectory Balance Loss: (reward - lnZ + logp_prior_env)^2\n",
    "    loss = ((reward - lnZ_LLM_PRIOR_ENV + logp_LLM_PRIOR_ENV) ** 2).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_LLM_ADVERSERIAL(out_LLM_PRIOR_ENV, out_LLM_ADVERSERIAL, out_Seq2Seq):\n",
    "    \"\"\"\n",
    "    Compute the adversarial GFlowNet loss.\n",
    "\n",
    "    Args:\n",
    "        out_LLM_PRIOR_ENV: tuple(logp_prior_env (B,), lnZ_prior_env scalar)\n",
    "        out_LLM_ADVERSERIAL: tuple(logp_adverserial (B,), lnZ_adverserial scalar)\n",
    "        out_Seq2Seq: tuple(logp_seq2seq (B,), lnZ_seq2seq scalar)\n",
    "\n",
    "    Returns:\n",
    "        loss_LLM_ADVERSERIAL: scalar tensor (for backprop on LLM_ADVERSERIAL)\n",
    "    \"\"\"\n",
    "    logp_prior_env, lnZ_prior_env = out_LLM_PRIOR_ENV\n",
    "    logp_adverserial, lnZ_adverserial = out_LLM_ADVERSERIAL\n",
    "    logp_seq2seq, _ = out_Seq2Seq\n",
    "\n",
    "    # D√©tachement des termes exog√®nes\n",
    "    logp_prior_env_detached = logp_prior_env.detach()\n",
    "    logp_seq2seq_detached = logp_seq2seq.detach()\n",
    "\n",
    "    # Adversarial loss = (reward)^(-2)\n",
    "    loss = (logp_prior_env_detached + logp_seq2seq_detached + logp_adverserial - lnZ_adverserial + logp_adverserial).pow(-2).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def loss_LLM_RENFORCEMENT(out_LLM_PRIOR_ENV, out_LLM_ADVERSERIAL, out_Seq2Seq):\n",
    "    \"\"\"\n",
    "    Compute the reinforcement (Seq-to-Seq) loss.\n",
    "\n",
    "    Args:\n",
    "        out_LLM_PRIOR_ENV: tuple(logp_prior_env (B,), lnZ_prior_env scalar)\n",
    "        out_LLM_ADVERSERIAL: tuple(logp_adversarial (B,), lnZ_adversarial scalar)\n",
    "        out_Seq2Seq: tuple(logp_seq2seq (B,), lnZ_seq2seq (B,))\n",
    "\n",
    "    Returns:\n",
    "        loss_LLM_RENFORCEMENT: scalar tensor (for backprop on Seq-to-Seq)\n",
    "    \"\"\"\n",
    "    logp_prior_env, _ = out_LLM_PRIOR_ENV\n",
    "    logp_adversarial, _ = out_LLM_ADVERSERIAL\n",
    "    logp_seq2seq, lnZ_seq2seq = out_Seq2Seq\n",
    "\n",
    "    # D√©tachement des termes exog√®nes\n",
    "    logp_prior_env_detached = logp_prior_env.detach()\n",
    "    logp_adversarial_detached = logp_adversarial.detach()\n",
    "\n",
    "    # Reinforcement loss = (reward)^(2)\n",
    "    loss = (logp_prior_env_detached + logp_seq2seq + logp_adversarial_detached - lnZ_seq2seq + logp_seq2seq).pow(2).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44e03b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, n_layers=1, n_heads=1, dropout=0.1, lnZ_shape=()):\n",
    "        \"\"\"\n",
    "        Transformer Decoder pour LLM_PRIOR_ENV ou LLM_ADVERSERIAL.\n",
    "        Peut fonctionner en mode on-policy (g√©n√©ration) ou off-policy (log prob).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, n_heads, dim_feedforward=4*d_model, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.lnZ = nn.Parameter(torch.zeros(lnZ_shape, requires_grad=True))\n",
    "        self.d_model = d_model\n",
    "\n",
    "    # ======================================\n",
    "    #   UTILITAIRES MASQUAGE AUTOREGRESSIF\n",
    "    # ======================================\n",
    "    def _causal_mask(self, T):\n",
    "        \"\"\"Masque triangulaire inf√©rieur pour bloquer la fuite future (autoregressif).\"\"\"\n",
    "        return torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    # ======================================\n",
    "    #   FORWARD AUTOREGRESSIF\n",
    "    # ======================================\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: (B, T)\n",
    "        Sort: logits (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        x = self.embed(input_ids) * (self.d_model ** 0.5)\n",
    "        mask = self._causal_mask(T).to(x.device)\n",
    "        h = self.decoder(x, x, tgt_mask=mask)\n",
    "        return self.output(h)\n",
    "\n",
    "    # ======================================\n",
    "    #   MODE OFF-POLICY : log p(seq)\n",
    "    # ======================================\n",
    "    def log_prob(self, tokens):\n",
    "        \"\"\"\n",
    "        Calcule la log-probabilit√© totale d'une s√©quence donn√©e (off-policy).\n",
    "        tokens: (B, T)\n",
    "        Retourne: (logp_total (B,), lnZ)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        logits = self.forward(tokens[:, :-1])                 # (B, T-1, vocab)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        next_tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "        token_logp = logp.gather(2, next_tokens).squeeze(-1)  # (B, T-1)\n",
    "        logp_total = token_logp.sum(dim=1)                    # (B,)\n",
    "        return logp_total, self.lnZ\n",
    "\n",
    "    # ======================================\n",
    "    #   MODE ON-POLICY : g√©n√©ration par sampling\n",
    "    # ======================================\n",
    "    def generate(self, batch_size, max_len, bos_token_id, eos_token_id, device):\n",
    "        \"\"\"\n",
    "        G√©n√©ration batch√©e diff√©rentiable sur logp(seq).\n",
    "        Retourne: tokens (B, T_gen), logp_total (B,), lnZ\n",
    "        \"\"\"\n",
    "        tokens = torch.full((batch_size, 1), bos_token_id, dtype=torch.long, device=device)\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "        logp_total = torch.zeros(batch_size, device=device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = self.forward(tokens)[:, -1, :]        # (B, vocab)\n",
    "            logp = F.log_softmax(logits, dim=-1)           # (B, vocab)\n",
    "\n",
    "            next_tokens = torch.multinomial(logp.exp(), 1) # (B, 1)\n",
    "            next_logp = logp.gather(1, next_tokens).squeeze(1)\n",
    "\n",
    "            logp_total += next_logp * (~finished)\n",
    "            tokens = torch.cat([tokens, next_tokens], dim=1)\n",
    "\n",
    "            finished |= next_tokens.squeeze(1).eq(eos_token_id)\n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "        return tokens, logp_total, self.lnZ\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c18aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithCritic(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, n_layers=1, n_heads=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer Seq2Seq avec un critic (Z-network) qui renvoie lnZ(B,)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 20, d_model))  # sinusoidal-like positional encoding\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # --- Encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=4*d_model, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=n_heads,\n",
    "            dim_feedforward=4*d_model, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # --- Output head ---\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # --- Critic head (predicts lnZ_i par s√©quence) ---\n",
    "        self.critic = nn.Linear(d_model, 1)\n",
    "\n",
    "    # ====================================================\n",
    "    # Helper causal mask for autoregressive decoding\n",
    "    # ====================================================\n",
    "    def _causal_mask(self, T):\n",
    "        return torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    # ====================================================\n",
    "    # Forward (teacher-forced decoding)\n",
    "    # ====================================================\n",
    "    def forward(self, encoder_input_ids, decoder_input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_input_ids: (B, T_enc)\n",
    "            decoder_input_ids: (B, T_dec)\n",
    "        Returns:\n",
    "            logits: (B, T_dec, vocab)\n",
    "            lnZ: (B,)\n",
    "        \"\"\"\n",
    "        B, T_enc = encoder_input_ids.shape\n",
    "        B, T_dec = decoder_input_ids.shape\n",
    "\n",
    "        # Embedding + positional encoding\n",
    "        enc = self.embed(encoder_input_ids) * (self.d_model ** 0.5)\n",
    "        dec = self.embed(decoder_input_ids) * (self.d_model ** 0.5)\n",
    "        enc = enc + self.pos_embed[:, :T_enc, :]\n",
    "        dec = dec + self.pos_embed[:, :T_dec, :]\n",
    "\n",
    "        # Encoder\n",
    "        memory = self.encoder(enc)\n",
    "\n",
    "        # Critic : moyenne spatiale ‚Üí pr√©diction lnZ_i pour chaque s√©quence\n",
    "        memory_mean = memory.mean(dim=1)               # (B, d_model)\n",
    "        lnZ = self.critic(memory_mean).squeeze(1)      # (B,)\n",
    "\n",
    "        # Decoder autoregressif\n",
    "        mask = self._causal_mask(T_dec).to(dec.device)\n",
    "        h = self.decoder(dec, memory, tgt_mask=mask)\n",
    "\n",
    "        logits = self.output(h)  # (B, T_dec, vocab)\n",
    "        return logits, lnZ\n",
    "\n",
    "    # ====================================================\n",
    "    # Mode OFF-POLICY : log-probabilit√© exacte d'une s√©quence\n",
    "    # ====================================================\n",
    "    def log_prob(self, encoder_input_ids, full_target_ids):\n",
    "        \"\"\"\n",
    "        Calcule la log-prob totale de la s√©quence cible donn√©e.\n",
    "        \"\"\"\n",
    "        decoder_input_ids = full_target_ids[:, :-1]     # tokens d√©j√† vus (input)\n",
    "        target_ids = full_target_ids[:, 1:]             # tokens √† pr√©dire (labels)\n",
    "        \n",
    "        logits, lnZ = self.forward(encoder_input_ids, decoder_input_ids)\n",
    "        logp = F.log_softmax(logits, dim=-1)                       # (B, T_dec, vocab)\n",
    "        next_tokens = target_ids.unsqueeze(-1)                     # (B, T_dec, 1)\n",
    "        token_logp = logp.gather(2, next_tokens).squeeze(-1)       # (B, T_dec)\n",
    "        logp_total = token_logp.sum(dim=1)                         # (B,)\n",
    "        return logp_total, lnZ\n",
    "\n",
    "    # ====================================================\n",
    "    # Mode ON-POLICY : g√©n√©ration batch√©e\n",
    "    # ====================================================\n",
    "    def generate(self, encoder_input_ids, max_len, bos_token_id, eos_token_id, device):\n",
    "        B = encoder_input_ids.size(0)\n",
    "        dec_tokens = torch.full((B, 1), bos_token_id, dtype=torch.long, device=device)\n",
    "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "        logp_total = torch.zeros(B, device=device)\n",
    "\n",
    "        enc = self.embed(encoder_input_ids) * (self.d_model ** 0.5)\n",
    "        enc = enc + self.pos_embed[:, :encoder_input_ids.size(1), :]\n",
    "        memory = self.encoder(enc)\n",
    "\n",
    "        memory_mean = memory.mean(dim=1)\n",
    "        lnZ = self.critic(memory_mean).squeeze(1)  # critic d√©pend de l‚Äôentr√©e (B,)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            dec = self.embed(dec_tokens) * (self.d_model ** 0.5)\n",
    "            dec = dec + self.pos_embed[:, :dec_tokens.size(1), :]\n",
    "            mask = self._causal_mask(dec_tokens.size(1)).to(dec.device)\n",
    "            h = self.decoder(dec, memory, tgt_mask=mask)\n",
    "            logits = self.output(h)[:, -1, :]              # (B, vocab)\n",
    "            logp = F.log_softmax(logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(logp.exp(), 1) # (B, 1)\n",
    "            next_logp = logp.gather(1, next_tokens).squeeze(1)\n",
    "\n",
    "            logp_total += next_logp * (~finished)\n",
    "            dec_tokens = torch.cat([dec_tokens, next_tokens], dim=1)\n",
    "            finished |= next_tokens.squeeze(1).eq(eos_token_id)\n",
    "\n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "        return dec_tokens, logp_total, lnZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e55ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=64, num_layers=1, dropout=0.1, lnZ_shape=()):\n",
    "        \"\"\"\n",
    "        GRU Decoder-only pour LLM_PRIOR_ENV ou LLM_ADVERSERIAL (√©quivalent lightweight √† LLMDecoder).\n",
    "        Plug-and-play : m√™mes interfaces forward/log_prob/generate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(\n",
    "            hidden_size, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "        self.lnZ = nn.Parameter(torch.zeros(lnZ_shape, requires_grad=True))\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        input_ids: (B, T)\n",
    "        Retourne: logits (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        x = self.embed(input_ids)\n",
    "        out, _ = self.gru(x)\n",
    "        return self.output(out)\n",
    "\n",
    "    def log_prob(self, tokens):\n",
    "        \"\"\"\n",
    "        Calcule la log-probabilit√© totale d'une s√©quence donn√©e (off-policy).\n",
    "        tokens: (B, T)\n",
    "        Retourne: (logp_total (B,), lnZ)\n",
    "        \"\"\"\n",
    "        B, T = tokens.shape\n",
    "        logits = self.forward(tokens[:, :-1])                 # (B, T-1, vocab)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        next_tokens = tokens[:, 1:].unsqueeze(-1)\n",
    "        token_logp = logp.gather(2, next_tokens).squeeze(-1)  # (B, T-1)\n",
    "        logp_total = token_logp.sum(dim=1)                    # (B,)\n",
    "        return logp_total, self.lnZ\n",
    "\n",
    "    def generate(self, batch_size, max_len, bos_token_id, eos_token_id, device):\n",
    "        \"\"\"\n",
    "        G√©n√©ration batch√©e diff√©rentiable sur logp(seq).\n",
    "        Retourne: tokens (B, T_gen), logp_total (B,), lnZ\n",
    "        \"\"\"\n",
    "        tokens = torch.full((batch_size, 1), bos_token_id, dtype=torch.long, device=device)\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "        logp_total = torch.zeros(batch_size, device=device)\n",
    "        h = None  # Hidden initialis√© √† z√©ro par GRU\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            inp = tokens[:, -1:]  # (B, 1)\n",
    "            x = self.embed(inp)\n",
    "            out, h = self.gru(x, h)\n",
    "            logits = self.output(out[:, -1, :])               # (B, vocab)\n",
    "            logp = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            next_tokens = torch.multinomial(logp.exp(), 1)    # (B, 1)\n",
    "            next_logp = logp.gather(1, next_tokens).squeeze(1)\n",
    "\n",
    "            logp_total += next_logp * (~finished)\n",
    "            tokens = torch.cat([tokens, next_tokens], dim=1)\n",
    "\n",
    "            finished |= next_tokens.squeeze(1).eq(eos_token_id)\n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "        return tokens, logp_total, self.lnZ\n",
    "    \n",
    "\n",
    "class GRUSeq2SeqWithCritic(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=64, num_layers=1, dropout=0.1):\n",
    "        \"\"\"\n",
    "        GRU Seq2Seq avec critic (√©quivalent lightweight √† Seq2SeqWithCritic).\n",
    "        Plug-and-play : m√™mes interfaces forward/log_prob/generate.\n",
    "        Pas de positional encoding (r√©current g√®re l'ordre).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder_gru = nn.GRU(\n",
    "            hidden_size, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.decoder_gru = nn.GRU(\n",
    "            hidden_size, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, encoder_input_ids, decoder_input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_input_ids: (B, T_enc)\n",
    "            decoder_input_ids: (B, T_dec)\n",
    "        Returns:\n",
    "            logits: (B, T_dec, vocab)\n",
    "            lnZ: (B,)\n",
    "        \"\"\"\n",
    "        B, T_enc = encoder_input_ids.shape\n",
    "        B, T_dec = decoder_input_ids.shape\n",
    "\n",
    "        # Encoder\n",
    "        enc_x = self.embed(encoder_input_ids)\n",
    "        enc_out, enc_h = self.encoder_gru(enc_x)\n",
    "        memory_mean = enc_out.mean(dim=1)                     # (B, hidden)\n",
    "        lnZ = self.critic(memory_mean).squeeze(1)             # (B,)\n",
    "\n",
    "        # Decoder (teacher-forced)\n",
    "        dec_x = self.embed(decoder_input_ids)\n",
    "        dec_out, _ = self.decoder_gru(dec_x, enc_h)\n",
    "        logits = self.output(dec_out)                         # (B, T_dec, vocab)\n",
    "        return logits, lnZ\n",
    "\n",
    "    def log_prob(self, encoder_input_ids, full_target_ids):\n",
    "        \"\"\"\n",
    "        Calcule la log-prob totale de la s√©quence cible donn√©e.\n",
    "        \"\"\"\n",
    "        decoder_input_ids = full_target_ids[:, :-1]\n",
    "        target_ids = full_target_ids[:, 1:]\n",
    "        \n",
    "        logits, lnZ = self.forward(encoder_input_ids, decoder_input_ids)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        next_tokens = target_ids.unsqueeze(-1)\n",
    "        token_logp = logp.gather(2, next_tokens).squeeze(-1)\n",
    "        logp_total = token_logp.sum(dim=1)\n",
    "        return logp_total, lnZ\n",
    "\n",
    "    def generate(self, encoder_input_ids, max_len, bos_token_id, eos_token_id, device):\n",
    "        \"\"\"\n",
    "        G√©n√©ration batch√©e autoregressive.\n",
    "        Retourne: tokens (B, T_gen), logp_total (B,), lnZ (B,)\n",
    "        \"\"\"\n",
    "        B = encoder_input_ids.size(0)\n",
    "        dec_tokens = torch.full((B, 1), bos_token_id, dtype=torch.long, device=device)\n",
    "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
    "        logp_total = torch.zeros(B, device=device)\n",
    "\n",
    "        # Encoder une fois\n",
    "        enc_x = self.embed(encoder_input_ids)\n",
    "        enc_out, enc_h = self.encoder_gru(enc_x)\n",
    "        memory_mean = enc_out.mean(dim=1)\n",
    "        lnZ = self.critic(memory_mean).squeeze(1)\n",
    "\n",
    "        dec_h = enc_h  # Init decoder avec encoder hidden\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            inp = dec_tokens[:, -1:]  # (B, 1)\n",
    "            x = self.embed(inp)\n",
    "            dec_out, dec_h = self.decoder_gru(x, dec_h)\n",
    "            logits = self.output(dec_out[:, -1, :])           # (B, vocab)\n",
    "            logp = F.log_softmax(logits, dim=-1)\n",
    "            next_tokens = torch.multinomial(logp.exp(), 1)\n",
    "            next_logp = logp.gather(1, next_tokens).squeeze(1)\n",
    "\n",
    "            logp_total += next_logp * (~finished)\n",
    "            dec_tokens = torch.cat([dec_tokens, next_tokens], dim=1)\n",
    "            finished |= next_tokens.squeeze(1).eq(eos_token_id)\n",
    "\n",
    "            if finished.all():\n",
    "                break\n",
    "\n",
    "        return dec_tokens, logp_total, lnZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579c17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, file_path=\"addition_dataset_train.pkl\"):\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            self.data = pickle.load(f)[\"data\"]\n",
    "\n",
    "    def encode(self, text, max_len, add_bos_eos=False):\n",
    "        ids = [char2idx[ch] for ch in text]\n",
    "        if add_bos_eos:\n",
    "            ids = [char2idx[BOS]] + ids + [char2idx[EOS]]\n",
    "        ids = ids[:max_len]\n",
    "        ids += [char2idx[PAD]] * (max_len - len(ids))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, tgt = self.data[idx]\n",
    "        x = self.encode(inp, MAX_LEN_INP, add_bos_eos=True)\n",
    "        y = self.encode(tgt, MAX_LEN_OUT, add_bos_eos=True)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877b16e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayesian_information_offpolicy(seq2seq_model, llm_model, encoder_input_ids, decoder_input_ids, target_ids):\n",
    "    \"\"\"\n",
    "    Calcule KL(P_seq2seq || P_llm) et entropies normalis√©es (off-policy).\n",
    "    Args:\n",
    "        encoder_input_ids: (B, T_enc)\n",
    "        decoder_input_ids: (B, T_dec)\n",
    "        target_ids: (B, T_dec)\n",
    "    Returns:\n",
    "        kl: ()\n",
    "        H_norm: ()\n",
    "    \"\"\"\n",
    "    # log-probs (B,)\n",
    "    logp_seq2seq, _ = seq2seq_model.log_prob(encoder_input_ids, target_ids)\n",
    "    logp_llm, _ = llm_model.log_prob(decoder_input_ids)\n",
    "\n",
    "    # KL et entropies\n",
    "    kl = (logp_seq2seq - logp_llm).mean()          # ()\n",
    "    H_seq2seq = -(logp_seq2seq).mean()             # ()\n",
    "    H_llm = -(logp_llm).mean()                     # ()\n",
    "\n",
    "    # Normalisation\n",
    "    T_norm = torch.log(torch.tensor(decoder_input_ids.size(1), dtype=torch.float32, device=logp_seq2seq.device))\n",
    "    H_norm = (H_seq2seq + H_llm) / (2 * T_norm)    # ()\n",
    "\n",
    "    return kl, H_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "177f94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayesian_information_onpolicy(seq2seq_model, llm_model, bos_token_id, eos_token_id, max_len, device, B=16):\n",
    "    \"\"\"\n",
    "    KL(P_seq2seq || P_llm) et entropies normalis√©es (on-policy)\n",
    "    \"\"\"\n",
    "    # G√©n√©ration par Seq2Seq\n",
    "    with torch.no_grad():\n",
    "        encoder_input_ids = torch.randint(3, 50, (B, 8), device=device)  # dummy input encoder\n",
    "    seq2seq_tokens, logp_seq2seq, _ = seq2seq_model.generate(\n",
    "        encoder_input_ids, max_len, bos_token_id, eos_token_id, device\n",
    "    )\n",
    "\n",
    "    # √âvaluation du LLM sur les s√©quences g√©n√©r√©es\n",
    "    logp_llm, _ = llm_model.log_prob(seq2seq_tokens)\n",
    "\n",
    "    # KL divergence\n",
    "    kl = (logp_seq2seq - logp_llm).mean()\n",
    "\n",
    "    # Entropies brutes\n",
    "    H_seq2seq = -(logp_seq2seq).mean()\n",
    "    H_llm = -(logp_llm).mean()\n",
    "\n",
    "    # Normalisation par log(longueur moyenne des s√©quences g√©n√©r√©es)\n",
    "    avg_len = seq2seq_tokens.size(1)\n",
    "    T_norm = torch.log(torch.tensor(avg_len, dtype=torch.float32, device=logp_seq2seq.device))\n",
    "    H_norm = (H_seq2seq + H_llm) / (2 * T_norm)\n",
    "\n",
    "    return kl, H_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "337df90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL = [\"<pad>\", \"<bos>\", \"<eos>\"] # pad for padding, bos for beginning of sequence, eos for end of sequence\n",
    "BASE_CHARS = list(\"0123456789+= \")\n",
    "PAD, BOS, EOS = SPECIAL\n",
    "VOCAB = SPECIAL + BASE_CHARS\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "char2idx = {ch: i for i, ch in enumerate(VOCAB)}\n",
    "idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "\n",
    "MAX_LEN_INP = 13   # max length input (\"aaa + bbb =<EOS>\") for llm and \n",
    "MAX_LEN_OUT = 6    # max length output (\"xxx\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707a70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AdditionDataset(\"addition_dataset_train.pkl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4865259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:01<00:00, 43.87it/s, loss=9.11e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1/1 ‚Äî mean loss: 8527.744022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "llm_prior = GRUDecoder(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "seq2seq = GRUSeq2SeqWithCritic(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "seq2seq.eval()  # frozen for phase 1\n",
    "\n",
    "optimizer = optim.Adam(llm_prior.parameters(), lr=LR)\n",
    "\n",
    "# Loss (Trajectory Balance)\n",
    "def loss_LLM_PRIOR_ENV(out_LLM_PRIOR_ENV, out_Seq2Seq):\n",
    "    logp_LLM_PRIOR_ENV, lnZ_LLM_PRIOR_ENV = out_LLM_PRIOR_ENV\n",
    "    logp_Seq2Seq_detached = out_Seq2Seq.detach()\n",
    "    reward = logp_LLM_PRIOR_ENV + logp_Seq2Seq_detached\n",
    "    loss = ((reward - lnZ_LLM_PRIOR_ENV + logp_LLM_PRIOR_ENV) ** 2).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    llm_prior.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        x_in, y_out = [b.to(DEVICE) for b in batch]\n",
    "\n",
    "        # Compute log-probabilities\n",
    "        logp_prior, lnZ_prior = llm_prior.log_prob(x_in)  # (B,), scalar\n",
    "        with torch.no_grad():\n",
    "            logp_seq2seq, _ = seq2seq.log_prob(\n",
    "                encoder_input_ids=x_in,\n",
    "                full_target_ids=x_in[:, 1:]\n",
    "            )\n",
    "\n",
    "        # Loss + backprop\n",
    "        loss = loss_LLM_PRIOR_ENV((logp_prior, lnZ_prior), logp_seq2seq)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(llm_prior.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": running_loss / (pbar.n + 1)})\n",
    "\n",
    "    print(f\"‚úÖ Epoch {epoch+1}/{EPOCHS} ‚Äî mean loss: {running_loss / len(train_loader):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a56549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Adversarial] Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:01<00:00, 40.71it/s, loss=6.05e-5] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [Adversarial] Epoch 1 ‚Äî mean loss: 0.000058\n",
      "üíæ Saved model to llm_adversarial_offpolicy.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# PHASE 2: LLM ADVERSARIAL TRAINING\n",
    "# ======================================\n",
    "\n",
    "llm_adv = LLMDecoder(vocab_size=VOCAB_SIZE).to(DEVICE)\n",
    "optimizer_adv = optim.Adam(llm_adv.parameters(), lr=LR)\n",
    "\n",
    "# Adversarial loss\n",
    "def loss_LLM_ADVERSARIAL(out_LLM_PRIOR_ENV, out_LLM_ADVERSARIAL, out_Seq2Seq):\n",
    "    logp_prior_env, lnZ_prior_env = out_LLM_PRIOR_ENV\n",
    "    logp_adversarial, lnZ_adversarial = out_LLM_ADVERSARIAL\n",
    "    logp_seq2seq, _ = out_Seq2Seq\n",
    "\n",
    "    logp_prior_env_det = logp_prior_env.detach()\n",
    "    logp_seq2seq_det = logp_seq2seq.detach()\n",
    "\n",
    "    loss = (\n",
    "        logp_prior_env_det\n",
    "        + logp_seq2seq_det\n",
    "        + logp_adversarial\n",
    "        - lnZ_adversarial\n",
    "        + logp_adversarial\n",
    "    ).pow(-2).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    llm_adv.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"[Adversarial] Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        x_in, y_out = [b.to(DEVICE) for b in batch]\n",
    "\n",
    "        # Forward: prior, adversarial, seq2seq\n",
    "        logp_prior, lnZ_prior = llm_prior.log_prob(x_in)\n",
    "        logp_adv, lnZ_adv = llm_adv.log_prob(x_in)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logp_seq2seq, _ = seq2seq.log_prob(\n",
    "                encoder_input_ids=x_in,\n",
    "                full_target_ids=x_in[:, 1:]\n",
    "            )\n",
    "\n",
    "        loss = loss_LLM_ADVERSARIAL(\n",
    "            (logp_prior, lnZ_prior),\n",
    "            (logp_adv, lnZ_adv),\n",
    "            (logp_seq2seq, None)\n",
    "        )\n",
    "\n",
    "        optimizer_adv.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(llm_adv.parameters(), 1.0)\n",
    "        optimizer_adv.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": running_loss / (pbar.n + 1)})\n",
    "\n",
    "    print(f\"‚úÖ [Adversarial] Epoch {epoch+1} ‚Äî mean loss: {running_loss / len(train_loader):.6f}\")\n",
    "\n",
    "torch.save(llm_adv.state_dict(), \"llm_adversarial_offpolicy.pt\")\n",
    "print(\"üíæ Saved model to llm_adversarial_offpolicy.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90d17efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Reinforce] Epoch 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:01<00:00, 39.61it/s, loss=1.53e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ [Reinforce] Epoch 1 ‚Äî mean loss: 14912.064328\n",
      "üíæ Saved model to seq2seq_reinforce.pt\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# PHASE 3: SEQ-TO-SEQ REINFORCEMENT TRAINING\n",
    "# ======================================\n",
    "\n",
    "optimizer_seq2seq = optim.Adam(seq2seq.parameters(), lr=LR)\n",
    "\n",
    "def loss_LLM_REINFORCEMENT(out_LLM_PRIOR_ENV, out_LLM_ADVERSARIAL, out_Seq2Seq):\n",
    "    logp_prior_env, _ = out_LLM_PRIOR_ENV\n",
    "    logp_adversarial, _ = out_LLM_ADVERSARIAL\n",
    "    logp_seq2seq, lnZ_seq2seq = out_Seq2Seq\n",
    "\n",
    "    logp_prior_env_det = logp_prior_env.detach()\n",
    "    logp_adversarial_det = logp_adversarial.detach()\n",
    "\n",
    "    loss = (\n",
    "        logp_prior_env_det\n",
    "        + logp_seq2seq\n",
    "        + logp_adversarial_det\n",
    "        - lnZ_seq2seq\n",
    "        + logp_seq2seq\n",
    "    ).pow(2).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    seq2seq.train()\n",
    "    running_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=f\"[Reinforce] Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch in pbar:\n",
    "        x_in, y_out = [b.to(DEVICE) for b in batch]\n",
    "\n",
    "        # Forward passes\n",
    "        logp_prior, lnZ_prior = llm_prior.log_prob(x_in)\n",
    "        logp_adv, lnZ_adv = llm_adv.log_prob(x_in)\n",
    "        logp_seq2seq, lnZ_seq2seq = seq2seq.log_prob(\n",
    "            encoder_input_ids=x_in[:, :-1],\n",
    "            full_target_ids=x_in[:, 1:]\n",
    "        )\n",
    "\n",
    "        loss = loss_LLM_REINFORCEMENT(\n",
    "            (logp_prior, lnZ_prior),\n",
    "            (logp_adv, lnZ_adv),\n",
    "            (logp_seq2seq, lnZ_seq2seq)\n",
    "        )\n",
    "\n",
    "        optimizer_seq2seq.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), 1.0)\n",
    "        optimizer_seq2seq.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": running_loss / (pbar.n + 1)})\n",
    "\n",
    "    print(f\"‚úÖ [Reinforce] Epoch {epoch+1} ‚Äî mean loss: {running_loss / len(train_loader):.6f}\")\n",
    "\n",
    "torch.save(seq2seq.state_dict(), \"seq2seq_reinforce.pt\")\n",
    "print(\"üíæ Saved model to seq2seq_reinforce.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f6d105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CHECKPOINT MANAGER\n",
    "# ======================================================\n",
    "\n",
    "def save_checkpoint(epoch, llm_prior, llm_adv, seq2seq, opt_prior, opt_adv, opt_seq2seq, metrics, path=\"checkpoints\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"llm_prior\": llm_prior.state_dict(),\n",
    "        \"llm_adv\": llm_adv.state_dict(),\n",
    "        \"seq2seq\": seq2seq.state_dict(),\n",
    "        \"opt_prior\": opt_prior.state_dict(),\n",
    "        \"opt_adv\": opt_adv.state_dict(),\n",
    "        \"opt_seq2seq\": opt_seq2seq.state_dict(),\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "    torch.save(ckpt, os.path.join(path, f\"epoch_{epoch}.pt\"))\n",
    "    # Save metrics log\n",
    "    with open(os.path.join(path, \"log.json\"), \"a\") as f:\n",
    "        json.dump({\"epoch\": epoch, **metrics}, f)\n",
    "        f.write(\"\\n\")\n",
    "    print(f\"üíæ Saved checkpoint for epoch {epoch}\")\n",
    "\n",
    "def load_latest_checkpoint(llm_prior, llm_adv, seq2seq, opt_prior, opt_adv, opt_seq2seq, path=\"checkpoints\"):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    ckpts = [f for f in os.listdir(path) if f.endswith(\".pt\")]\n",
    "    if not ckpts:\n",
    "        return 0\n",
    "    ckpts.sort(key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "    last_ckpt = os.path.join(path, ckpts[-1])\n",
    "    data = torch.load(last_ckpt, map_location=DEVICE)\n",
    "    llm_prior.load_state_dict(data[\"llm_prior\"])\n",
    "    llm_adv.load_state_dict(data[\"llm_adv\"])\n",
    "    seq2seq.load_state_dict(data[\"seq2seq\"])\n",
    "    opt_prior.load_state_dict(data[\"opt_prior\"])\n",
    "    opt_adv.load_state_dict(data[\"opt_adv\"])\n",
    "    opt_seq2seq.load_state_dict(data[\"opt_seq2seq\"])\n",
    "    print(f\"üîÅ Resumed from checkpoint: {ckpts[-1]}\")\n",
    "    return data[\"epoch\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242cdc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_from_logp(logp, normalize=True):\n",
    "    \"\"\"\n",
    "    Approximate the (normalized) entropy from log-probabilities of sequences.\n",
    "    \n",
    "    Args:\n",
    "        logp: torch.Tensor (B,) \n",
    "            Log-probabilities of each sequence in the batch (log p(seq))\n",
    "        normalize: bool\n",
    "            If True, normalize entropy by log(batch_size)\n",
    "    \n",
    "    Returns:\n",
    "        entropy: torch.Tensor (scalar)\n",
    "            Estimated (normalized) entropy value\n",
    "    \"\"\"\n",
    "    # Convert to probability (avoid overflow by clamping)\n",
    "    p = torch.exp(logp - logp.max())  # numerical stability\n",
    "    p = p / p.sum(dim=0, keepdim=True)  # normalize to form a distribution\n",
    "    \n",
    "    # Entropy = - Œ£ p * log p\n",
    "    H = -torch.sum(p * torch.log(p + 1e-12))\n",
    "    \n",
    "    # Normalize by log(batch_size) (so entropy ‚àà [0, 1])\n",
    "    if normalize:\n",
    "        H = H / torch.log(torch.tensor(float(len(logp)), device=logp.device) + 1e-9)\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a63dd288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33marthurmaffre\u001b[0m (\u001b[33marthurmaffre-alone\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/arthur/Desktop/GFlowNet_LLM_Bayes/wandb/run-20251128_095400-v5b7imv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence/runs/v5b7imv8' target=\"_blank\">run_20251128_095359_mps</a></strong> to <a href='https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence' target=\"_blank\">https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence/runs/v5b7imv8' target=\"_blank\">https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence/runs/v5b7imv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/arthurmaffre-alone/GFlowNet-Seq2Seq-Bayesian-Coherence/runs/v5b7imv8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x17e479010>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "SAVE_EVERY = 2\n",
    "# Initialize Weights & Biases\n",
    "wandb.init(\n",
    "    project=\"GFlowNet-Seq2Seq-Bayesian-Coherence\",\n",
    "    name=f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{DEVICE}\",\n",
    "    config={\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": 3e-4,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"device\": DEVICE,\n",
    "        \"save_every\": SAVE_EVERY\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b49693ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_and_plot_wandb(metrics, epoch):\n",
    "    wandb.log({\n",
    "        \"Loss/L_prior\": metrics[\"L_prior\"],\n",
    "        \"Loss/L_adv\": metrics[\"L_adv\"],\n",
    "        \"Loss/L_reinf\": metrics[\"L_reinf\"],\n",
    "        \"Entropy/H_adv\": metrics[\"H_adv\"],\n",
    "        \"Entropy/H_seq\": metrics[\"H_seq\"],\n",
    "        # \"KL/KL_prior_posterior\": metrics[\"KL_prior_posterior\"],\n",
    "        \"epoch\": epoch\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d95f5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_prior_posterior(train_loader, llm_prior, seq2seq, device):\n",
    "    \"\"\"\n",
    "    Approximate KL divergence between prior p_prior and posterior p_posterior ‚àù p_prior * p_seq2seq.\n",
    "    \"\"\"\n",
    "    kl_values = []\n",
    "    for batch in train_loader:\n",
    "        x_in, _ = [b.to(device) for b in batch]\n",
    "        with torch.no_grad():\n",
    "            logp_prior, _ = llm_prior.log_prob(x_in)\n",
    "            logp_seq2seq, _ = seq2seq.log_prob(\n",
    "                encoder_input_ids=x_in,\n",
    "                decoder_input_ids=x_in[:, :-1],\n",
    "                target_ids=x_in[:, 1:]\n",
    "            )\n",
    "        # Posterior (unnormalized)\n",
    "        logp_post = logp_prior + logp_seq2seq\n",
    "        # KL(p_post || p_prior) ‚âà E[log p_post - log p_prior] = E[log p_seq2seq]\n",
    "        kl = (logp_post - logp_prior).mean().item()\n",
    "        kl_values.append(kl)\n",
    "    return float(torch.tensor(kl_values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "487ce45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# TOKEN DECODER UTILITY\n",
    "# ======================================================\n",
    "\n",
    "def decode(tokens):\n",
    "    \"\"\"\n",
    "    Convertit un tenseur de tokens en texte lisible.\n",
    "    Exemple : [<bos>, '1', '2', '+', '3', '=', <eos>] ‚Üí '12+3='\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for seq in tokens:\n",
    "        text = \"\".join(\n",
    "            idx2char[int(i)]\n",
    "            for i in seq\n",
    "            if idx2char[int(i)] not in [\"<pad>\"]\n",
    "        )\n",
    "        texts.append(\n",
    "            text.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "\n",
    "def train_epoch_three_stages(\n",
    "    train_loader,\n",
    "    llm_prior, llm_adv, seq2seq,\n",
    "    opt_seq2seq_phase2, opt_adv, opt_seq2seq_phase4, opt_prior_phase1,\n",
    "    device,\n",
    "    dataset=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Effectue un cycle complet d'entra√Ænement :\n",
    "    1. Flow Matching (Prior)\n",
    "    2. Seq2Seq Reinforcement (on-policy)\n",
    "    3. Adversarial Divergence\n",
    "    4. Off-Policy Seq2Seq (pond√©r√© par le Prior)\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {}\n",
    "    \n",
    "    # ======================================================\n",
    "    # 4Ô∏è‚É£ PHASE OFF-POLICY ‚Äî Seq2Seq training on real data weighted by Prior\n",
    "    # ======================================================\n",
    "    seq2seq.train()\n",
    "    llm_prior.eval()\n",
    "    llm_adv.eval()\n",
    "\n",
    "    total_loss, total_H = 0.0, 0.0\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_passes = 1  # üëà combien de fois tu veux repasser sur le dataset\n",
    "\n",
    "    for pass_id in range(num_passes):\n",
    "        pbar = tqdm(train_loader, desc=f\"Phase 4 ‚Äî OFF-POLICY pass {pass_id+1}/{num_passes}\")\n",
    "        for batch in pbar:\n",
    "            x_in, y_out = [b.to(device) for b in batch]\n",
    "\n",
    "            # Compute prior probability on inputs (off-policy weighting)\n",
    "            with torch.no_grad():\n",
    "                logp_prior, _ = llm_prior.log_prob(x_in)\n",
    "\n",
    "            # Seq2Seq log-prob under its own model\n",
    "            logp_seq2seq, _ = seq2seq.log_prob(\n",
    "                encoder_input_ids=x_in,\n",
    "                full_target_ids=y_out\n",
    "            )\n",
    "\n",
    "            # Weighted loss (off-policy correction)\n",
    "            loss_4 = - (logp_seq2seq).mean()  # weighted NLL\n",
    "\n",
    "            opt_seq2seq_phase4.zero_grad()\n",
    "            loss_4.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), 1.0)\n",
    "            opt_seq2seq_phase4.step()\n",
    "\n",
    "            total_loss += loss_4.item()\n",
    "            pbar.set_postfix({\"L_offpolicy\": total_loss / (pbar.n + 1)})\n",
    "\n",
    "    metrics[\"L_offpolicy\"] = total_loss / len(train_loader)\n",
    "    wandb.log({\"L_offpolicy\": metrics[\"L_offpolicy\"]})\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # 1Ô∏è‚É£ PHASE PRIOR ENV ‚Äî Flow Matching (with adversarial regularization)\n",
    "    # ======================================================\n",
    "    llm_prior.train()\n",
    "    seq2seq.eval()\n",
    "    llm_adv.eval()\n",
    "\n",
    "    total_loss, total_H = 0.0, 0.0\n",
    "    pbar = tqdm(train_loader, desc=\"Phase 1 ‚Äî LLM PRIOR ENV (flow matching + adv)\")\n",
    "\n",
    "    alpha = 1  # importance du seq2seq reward\n",
    "    beta_entropy = 1e-3  # r√©gularisation d'entropie\n",
    "\n",
    "    for batch in pbar:\n",
    "        x_in, _ = [b.to(device) for b in batch]\n",
    "        if x_in.shape != torch.Size([128]):\n",
    "            continue\n",
    "\n",
    "        # Compute log probs under the prior\n",
    "        _, logp_prior, lnZ_prior = llm_prior.generate(batch_size=train_loader.batch_size,\n",
    "            max_len=MAX_LEN_INP,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Reward from seq2seq (proxy for p*(y|x))\n",
    "            logp_seq2seq, _ = seq2seq.log_prob(\n",
    "                encoder_input_ids=x_in,\n",
    "                full_target_ids=x_in[:, :-1]\n",
    "            )\n",
    "            # Pressure from adversarial model\n",
    "            logp_adv, _ = llm_adv.log_prob(x_in)\n",
    "\n",
    "        # --- Flow matching target with adversarial term ---\n",
    "        energy = -logp_prior -logp_seq2seq.detach()\n",
    "        loss_1 = (logp_prior + energy + lnZ_prior).pow(2).mean()\n",
    "\n",
    "        # --- Loss ---\n",
    "\n",
    "        # --- Optimization ---\n",
    "        opt_prior_phase1.zero_grad()\n",
    "        loss_1.backward()\n",
    "        opt_prior_phase1.step()\n",
    "\n",
    "        total_loss += loss_1.item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"L_prior\": total_loss / (pbar.n + 1)\n",
    "        })\n",
    "\n",
    "    metrics[\"L_prior\"] = total_loss / len(train_loader)\n",
    "    metrics[\"H_prior\"] = total_H / len(train_loader)\n",
    "    wandb.log({\n",
    "        \"L_prior\": metrics[\"L_prior\"],\n",
    "        \"H_prior\": metrics[\"H_prior\"],\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # 2Ô∏è‚É£ PHASE REINFORCEMENT ‚Äî Weighted Flow Matching\n",
    "    # ======================================================\n",
    "    seq2seq.train()\n",
    "    llm_prior.eval()\n",
    "    llm_adv.eval()\n",
    "\n",
    "    total_loss, total_H = 0.0, 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    pbar = tqdm(range(num_batches), desc=\"Phase 2 ‚Äî SEQ2SEQ REINFORCE (weighted)\")\n",
    "\n",
    "    beta_entropy = 1e-3\n",
    "\n",
    "    for _ in pbar:\n",
    "        tokens_adv, logp_adv, _ = llm_adv.generate(\n",
    "            batch_size=train_loader.batch_size,\n",
    "            max_len=MAX_LEN_INP,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        seq_tokens, logp_seq2seq, lnZ_seq2seq = seq2seq.generate(\n",
    "            encoder_input_ids=tokens_adv,\n",
    "            max_len=MAX_LEN_OUT,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logp_prior, _ = llm_prior.log_prob(tokens_adv)\n",
    "\n",
    "        # --- importance weighting ---\n",
    "\n",
    "        # --- flow-matching loss ---\n",
    "        \n",
    "        #weight = logp_prior.detach() - logp_adv.detach()\n",
    "        reward = logp_prior.detach() + logp_seq2seq\n",
    "\n",
    "        loss_2 = - reward.mean()\n",
    "\n",
    "        opt_seq2seq_phase2.zero_grad()\n",
    "        loss_2.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), 1.0)\n",
    "        opt_seq2seq_phase2.step()\n",
    "\n",
    "        total_loss += loss_2.item()\n",
    "        pbar.set_postfix({\n",
    "            \"L_reinf\": total_loss / (pbar.n + 1),\n",
    "            \"H_seq\": total_H / (pbar.n + 1)\n",
    "        })\n",
    "\n",
    "    metrics[\"L_reinf\"] = total_loss / num_batches\n",
    "    metrics[\"H_seq\"] = total_H / num_batches\n",
    "    wandb.log({\"L_reinf\": metrics[\"L_reinf\"], \"H_seq\": metrics[\"H_seq\"]})\n",
    "    \n",
    "    \n",
    "    # ======================================================\n",
    "    # 3Ô∏è‚É£ PHASE ADVERSARIAL ‚Äî Divergence Amplification\n",
    "    # ======================================================\n",
    "    llm_adv.train()\n",
    "    llm_prior.eval()\n",
    "    seq2seq.eval()\n",
    "\n",
    "    total_loss, total_H = 0.0, 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    pbar = tqdm(range(num_batches), desc=\"Phase 3 ‚Äî LLM ADVERSARIAL (divergence)\")\n",
    "\n",
    "    for _ in pbar:\n",
    "        tokens_adv, logp_adv, lnZ_adv = llm_adv.generate(\n",
    "            batch_size=train_loader.batch_size,\n",
    "            max_len=MAX_LEN_INP,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logp_prior, lnZ_prior = llm_prior.log_prob(tokens_adv)\n",
    "            logp_seq2seq, _ = seq2seq.log_prob(\n",
    "                encoder_input_ids=tokens_adv,\n",
    "                full_target_ids=tokens_adv[:, 1:]\n",
    "            )\n",
    "\n",
    "        # Weighting (importance correction)\n",
    "        weight = torch.exp(logp_prior.detach() - logp_adv.detach())\n",
    "        weight = weight / weight.sum()\n",
    "        # Divergence penalty (breaks coherence)\n",
    "        divergence_term = - logp_seq2seq.detach() - logp_prior.detach()\n",
    "\n",
    "        loss_3 = (weight * (lnZ_adv + logp_adv - divergence_term).pow(2)).mean()\n",
    "\n",
    "        opt_adv.zero_grad()\n",
    "        loss_3.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(llm_adv.parameters(), 1.0)\n",
    "        opt_adv.step()\n",
    "\n",
    "        total_loss += loss_3.item()\n",
    "        total_H += entropy_from_logp(logp_adv).item()\n",
    "        pbar.set_postfix({\n",
    "            \"L_adv\": total_loss / (pbar.n + 1),\n",
    "            \"H_adv\": total_H / (pbar.n + 1)\n",
    "        })\n",
    "\n",
    "    metrics[\"L_adv\"] = total_loss / num_batches\n",
    "    metrics[\"H_adv\"] = total_H / num_batches\n",
    "    wandb.log({\"L_adv\": metrics[\"L_adv\"], \"H_adv\": metrics[\"H_adv\"]})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # ======================================================\n",
    "    # 4Ô∏è‚É£ KL Divergence between Prior and Posterior\n",
    "    # ======================================================\n",
    "    # kl_post = kl_prior_posterior(train_loader, llm_prior, seq2seq, device)\n",
    "    # metrics[\"KL_prior_posterior\"] = kl_post\n",
    "    # wandb.log({\"KL_prior_posterior\": kl_post})\n",
    "    # print(f\"üåå KL(Prior ‚Äñ Posterior): {kl_post:.6f}\")\n",
    "\n",
    "\n",
    "    # ======================================================\n",
    "    # 5Ô∏è‚É£ GENERATION EXAMPLE (display + log to W&B)\n",
    "    # ======================================================\n",
    "    with torch.no_grad():\n",
    "        if dataset is not None:\n",
    "            x, y = dataset[torch.randint(0, len(dataset), (1,)).item()]\n",
    "            x, y = x.unsqueeze(0).to(device), y.unsqueeze(0).to(device)\n",
    "\n",
    "            prior_tokens, _, _ = llm_prior.generate(\n",
    "                batch_size=1,\n",
    "                max_len=MAX_LEN_INP,\n",
    "                bos_token_id=char2idx[\"<bos>\"],\n",
    "                eos_token_id=char2idx[\"<eos>\"],\n",
    "                device=device\n",
    "            )\n",
    "            seq_tokens, _, _ = seq2seq.generate(\n",
    "                encoder_input_ids=x[:, 1:],\n",
    "                max_len=MAX_LEN_OUT,\n",
    "                bos_token_id=char2idx[\"<bos>\"],\n",
    "                eos_token_id=char2idx[\"<eos>\"],\n",
    "                device=device\n",
    "            )\n",
    "            adv_tokens, _, _ = llm_adv.generate(\n",
    "                batch_size=1,\n",
    "                max_len=MAX_LEN_INP,\n",
    "                bos_token_id=char2idx[\"<bos>\"],\n",
    "                eos_token_id=char2idx[\"<eos>\"],\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            # D√©codage en texte\n",
    "            input_str, target_str = decode(x)[0], decode(y)[0]\n",
    "            prior_str, seq_str, adv_str = decode(prior_tokens)[0], decode(seq_tokens)[0], decode(adv_tokens)[0]\n",
    "\n",
    "            print(\"\\nüß† Example Generation:\")\n",
    "            print(f\"Input     : {input_str}\")\n",
    "            print(f\"Target    : {target_str}\")\n",
    "            print(f\"Prior Gen : {prior_str}\")\n",
    "            print(f\"Seq2Seq   : {seq_str}\")\n",
    "            print(f\"Adv Gen   : {adv_str}\")\n",
    "\n",
    "            wandb.log({\n",
    "                \"Generations/Sample\": wandb.Table(\n",
    "                    columns=[\"Input\", \"Target\", \"Prior\", \"Seq2Seq\", \"Adversarial\"],\n",
    "                    data=[[input_str, target_str, prior_str, seq_str, adv_str]],\n",
    "                )\n",
    "            })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dea7f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# CONFIG\n",
    "# ======================================================\n",
    "EPOCHS = 1\n",
    "SAVE_EVERY = 2\n",
    "CHECKPOINT_PATH = \"checkpoints\"\n",
    "NEW_MODEL = True  # ‚¨ÖÔ∏è change √† False pour reprendre le training\n",
    "DEVICE = \"mps\"\n",
    "SPECIAL = [\"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "BASE_CHARS = list(\"0123456789+=\")\n",
    "PAD, BOS, EOS = SPECIAL\n",
    "VOCAB = SPECIAL + BASE_CHARS\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "# ======================================================\n",
    "# INITIALIZE / RESET MODELS\n",
    "# ======================================================\n",
    "def init_models_and_optimizers(vocab_size):\n",
    "    llm_prior = LLMDecoder(vocab_size).to(DEVICE)\n",
    "    llm_adv = LLMDecoder(vocab_size).to(DEVICE)\n",
    "    seq2seq = Seq2SeqWithCritic(vocab_size).to(DEVICE)\n",
    "\n",
    "    # --- Optimizers s√©par√©s par phase ---\n",
    "    opt_prior_phase1 = torch.optim.Adam(llm_prior.parameters(), lr=4e-5)\n",
    "    opt_adv = torch.optim.Adam(llm_adv.parameters(), lr=4e-5)\n",
    "    opt_seq2seq_phase2 = torch.optim.Adam(seq2seq.parameters(), lr=4e-5)\n",
    "    opt_seq2seq_phase4 = torch.optim.Adam(seq2seq.parameters(), lr=4e-5)  # off-policy\n",
    "\n",
    "    return (\n",
    "        llm_prior, llm_adv, seq2seq,\n",
    "        opt_prior_phase1, opt_adv,\n",
    "        opt_seq2seq_phase2, opt_seq2seq_phase4\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8dbea39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthur/miniconda3/envs/Lucide/lib/python3.14/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting fresh training ‚Äî resetting models and optimizers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 4 ‚Äî OFF-POLICY pass 1/1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:03<00:00, 25.03it/s, L_offpolicy=14.5]\n",
      "Phase 1 ‚Äî LLM PRIOR ENV (flow matching + adv): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:00<00:00, 340.40it/s]\n",
      "Phase 2 ‚Äî SEQ2SEQ REINFORCE (weighted): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:28<00:00,  2.75it/s, L_reinf=49.3, H_seq=0]\n",
      "Phase 3 ‚Äî LLM ADVERSARIAL (divergence): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 78/78 [00:17<00:00,  4.47it/s, L_adv=86.4, H_adv=0.423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Example Generation:\n",
      "Input     : 69+26=\n",
      "Target    : 95\n",
      "Prior Gen : \n",
      "Seq2Seq   : +836\n",
      "Adv Gen   : 9882469\n",
      "\n",
      "Epoch 1/1 ‚Äî L_prior=0.0000 | L_adv=86.4387 | L_reinf=49.2809 | L_offpolicy=13.8989 | H_adv=0.4227 | H_seq=0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ======================================================\n",
    "# MAIN TRAIN LOOP\n",
    "# ======================================================\n",
    "\n",
    "# (Re)init models\n",
    "(\n",
    "    llm_prior, llm_adv, seq2seq,\n",
    "    opt_prior_phase1, opt_adv,\n",
    "    opt_seq2seq_phase2, opt_seq2seq_phase4\n",
    ") = init_models_and_optimizers(VOCAB_SIZE)\n",
    "\n",
    "if NEW_MODEL:\n",
    "    print(\"üöÄ Starting fresh training ‚Äî resetting models and optimizers.\")\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(CHECKPOINT_PATH):\n",
    "        for f in os.listdir(CHECKPOINT_PATH):\n",
    "            os.remove(os.path.join(CHECKPOINT_PATH, f))\n",
    "else:\n",
    "    print(\"üîÅ Attempting to resume from last checkpoint...\")\n",
    "    start_epoch = load_latest_checkpoint(\n",
    "        llm_prior, llm_adv, seq2seq,\n",
    "        opt_prior_phase1, opt_adv,\n",
    "        opt_seq2seq_phase2, opt_seq2seq_phase4,\n",
    "        path=CHECKPOINT_PATH\n",
    "    )\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    metrics = train_epoch_three_stages(\n",
    "        train_loader=train_loader,\n",
    "        llm_prior=llm_prior, llm_adv=llm_adv, seq2seq=seq2seq, opt_adv=opt_adv,\n",
    "        opt_seq2seq_phase2=opt_seq2seq_phase2, opt_seq2seq_phase4=opt_seq2seq_phase4,\n",
    "        device=DEVICE, opt_prior_phase1=opt_prior_phase1,\n",
    "        dataset=train_dataset\n",
    "    )\n",
    "\n",
    "    log_and_plot_wandb(metrics, epoch + 1)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch+1}/{EPOCHS} ‚Äî \"\n",
    "        f\"L_prior={metrics['L_prior']:.4f} | \"\n",
    "        f\"L_adv={metrics['L_adv']:.4f} | \"\n",
    "        f\"L_reinf={metrics['L_reinf']:.4f} | \"\n",
    "        f\"L_offpolicy={metrics['L_offpolicy']:.4f} | \"\n",
    "        f\"H_adv={metrics['H_adv']:.4f} | \"\n",
    "        f\"H_seq={metrics['H_seq']:.4f}\"\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % SAVE_EVERY == -1:\n",
    "        save_checkpoint(\n",
    "            epoch,\n",
    "            llm_prior, llm_adv, seq2seq,\n",
    "            opt_prior_phase1, opt_adv,\n",
    "            opt_seq2seq_phase2, opt_seq2seq_phase4,\n",
    "            metrics\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e075adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üß†  GENERATION EXAMPLES\n",
      "==============================\n",
      "\n",
      "--- Example 1 ---\n",
      "Input      : 37+71=\n",
      "Target     : 108\n",
      "Prior Gen  : \n",
      "Seq2Seq Gen: 815310\n",
      "Adv Gen    : =32+89=1\n",
      "logp_prior=-1.54, logp_seq=-13.89, logp_adv=-34.36\n",
      "lnZ_prior=0.00, lnZ_seq=-0.35, lnZ_adv=0.00\n",
      "\n",
      "--- Example 2 ---\n",
      "Input      : 66+72=\n",
      "Target     : 138\n",
      "Prior Gen  : \n",
      "Seq2Seq Gen: 905600\n",
      "Adv Gen    : 9=3+\n",
      "logp_prior=-6.68, logp_seq=-15.87, logp_adv=-19.06\n",
      "lnZ_prior=0.00, lnZ_seq=-0.32, lnZ_adv=0.00\n",
      "\n",
      "--- Example 3 ---\n",
      "Input      : 72+61=\n",
      "Target     : 133\n",
      "Prior Gen  : 3\n",
      "Seq2Seq Gen: 4+31=\n",
      "Adv Gen    : 01843639188\n",
      "logp_prior=-6.36, logp_seq=-14.48, logp_adv=-32.09\n",
      "lnZ_prior=0.00, lnZ_seq=-0.32, lnZ_adv=0.00\n",
      "\n",
      "--- Example 4 ---\n",
      "Input      : 7+60=\n",
      "Target     : 67\n",
      "Prior Gen  : 758+805+549\n",
      "Seq2Seq Gen: 8354\n",
      "Adv Gen    : \n",
      "logp_prior=-35.36, logp_seq=-13.54, logp_adv=-3.48\n",
      "lnZ_prior=0.00, lnZ_seq=-0.26, lnZ_adv=0.00\n",
      "\n",
      "--- Example 5 ---\n",
      "Input      : 37+79=\n",
      "Target     : 116\n",
      "Prior Gen  : 42++1313=0\n",
      "Seq2Seq Gen: =213+6\n",
      "Adv Gen    : =\n",
      "logp_prior=-34.39, logp_seq=-16.13, logp_adv=-8.03\n",
      "lnZ_prior=0.00, lnZ_seq=-0.31, lnZ_adv=0.00\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens):\n",
    "    \"\"\"Convert tensor of token ids -> readable string.\"\"\"\n",
    "    texts = []\n",
    "    for seq in tokens:\n",
    "        text = \"\".join(idx2char[int(i)] for i in seq if idx2char[int(i)] not in [\"<pad>\"])\n",
    "        texts.append(text.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\"))\n",
    "    return texts\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_generation_examples(llm_prior, llm_adv, seq2seq, dataset, device, n=5):\n",
    "    llm_prior.eval()\n",
    "    llm_adv.eval()\n",
    "    seq2seq.eval()\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"üß†  GENERATION EXAMPLES\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    for i in range(n):\n",
    "        x, y = dataset[i]\n",
    "        x, y = x.unsqueeze(0).to(device), y.unsqueeze(0).to(device)\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # LLM PRIOR ENV generation\n",
    "        # -----------------------------------------\n",
    "        prior_tokens, logp_prior, lnZ_prior = llm_prior.generate(\n",
    "            batch_size=1,\n",
    "            max_len=MAX_LEN_INP,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Seq2Seq generation (teacher-forced encoder)\n",
    "        # -----------------------------------------\n",
    "        encoder_input_ids = x[:, 1:]  # remove <bos> for encoder\n",
    "        seq_tokens, logp_seq, lnZ_seq = seq2seq.generate(\n",
    "            encoder_input_ids,\n",
    "            max_len=MAX_LEN_OUT,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # Optional: LLM ADVERSARIAL generation\n",
    "        # -----------------------------------------\n",
    "        adv_tokens, logp_adv, lnZ_adv = llm_adv.generate(\n",
    "            batch_size=1,\n",
    "            max_len=MAX_LEN_INP,\n",
    "            bos_token_id=char2idx[\"<bos>\"],\n",
    "            eos_token_id=char2idx[\"<eos>\"],\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(f\"Input      : {decode(x)[0]}\")\n",
    "        print(f\"Target     : {decode(y)[0]}\")\n",
    "        print(f\"Prior Gen  : {decode(prior_tokens)[0]}\")\n",
    "        print(f\"Seq2Seq Gen: {decode(seq_tokens)[0]}\")\n",
    "        print(f\"Adv Gen    : {decode(adv_tokens)[0]}\")\n",
    "        print(f\"logp_prior={logp_prior.item():.2f}, logp_seq={logp_seq.item():.2f}, logp_adv={logp_adv.item():.2f}\")\n",
    "        print(f\"lnZ_prior={lnZ_prior.item():.2f}, lnZ_seq={lnZ_seq.item():.2f}, lnZ_adv={lnZ_adv.item():.2f}\")\n",
    "\n",
    "\n",
    "show_generation_examples(llm_prior, llm_adv, seq2seq, dataset=train_dataset, device=DEVICE, n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lucide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
